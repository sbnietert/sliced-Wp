{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import numpy as np\n",
    "import ot\n",
    "from gsw.gsw import GSW\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y in R^{n x d}\n",
    "def proj_wp(X, Y, theta, p=2):\n",
    "    N, d = X.shape\n",
    "    theta = theta.flatten()\n",
    "    xproj = np.matmul(X, theta)\n",
    "    yproj = np.matmul(Y, theta)\n",
    "    return np.mean(np.abs((np.sort(xproj) - np.sort(yproj)))**p)**(1/p)\n",
    "\n",
    "def norm(x):\n",
    "    return np.sqrt(sum(x**2))\n",
    "\n",
    "def samp_sph(d):\n",
    "    x = np.random.normal(size = d)\n",
    "    return x/norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower bound W1 via the coupling that leaves points at 0 unmoved when possible\n",
    "def W1_lower_bounds(clean_data, filtered_data):\n",
    "    clean_norms = np.linalg.norm(clean_data, axis=1)\n",
    "    filtered_norms = np.linalg.norm(filtered_data, axis=1)\n",
    "    clean_nz = np.sum(clean_norms > 0)\n",
    "    clean_N = clean_data.shape[0]\n",
    "    clean_r = clean_nz/clean_N\n",
    "    filtered_nz = np.sum(filtered_norms > 0)\n",
    "    filtered_N = clean_data.shape[0]\n",
    "    filtered_r = filtered_nz/filtered_N\n",
    "\n",
    "    return (clean_r - filtered_r)*clean_norms[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subg_step(X, Y, theta, alpha):\n",
    "    N, d = X.shape\n",
    "    theta_X = np.matmul(X, theta)\n",
    "    theta_Y = np.matmul(Y, theta)\n",
    "\n",
    "    X_ind = np.argsort(theta_X)\n",
    "    Y_ind = np.argsort(theta_Y)\n",
    "    grad = 2*np.dot(theta_X[X_ind] - theta_Y[Y_ind], X[X_ind,:] - Y[Y_ind,:])/N\n",
    "    newtheta = (theta + alpha*grad)\n",
    "    return newtheta/norm(newtheta)\n",
    "             \n",
    "def msw2_distance_subg(X, Y, n_step, theta0):\n",
    "    N, d = X.shape\n",
    "    alpha = np.ones(n_step) # constant step size, can also try np.sqrt(range(1,n_step + 1))\n",
    "    theta = theta0\n",
    "    wp_dist = np.zeros(n_step)\n",
    "\n",
    "    time_iter = np.zeros(n_step+1)\n",
    "    U_iter = np.zeros((n_step+1,theta.shape[0],1))\n",
    "    U_iter[0,:,:] = theta0[np.newaxis].T\n",
    "\n",
    "    for i in range(n_step):\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        theta = subg_step(X, Y, theta, alpha[i])\n",
    "        wp_dist[i] = proj_wp(X, Y, theta)\n",
    "\n",
    "        toc = time.perf_counter()\n",
    "        time_iter[i + 1] = time_iter[i] + toc - tic\n",
    "        U_iter[i+1,:,:] = theta[np.newaxis].T\n",
    "    return proj_wp(X, Y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = 50\n",
    "lr = 1e-2\n",
    "p = 1.0\n",
    "\n",
    "dims = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
    "MSW_list = {}\n",
    "W_list = {}\n",
    "\n",
    "for d in dims:\n",
    "    print(f'd={d}')\n",
    "    # load data from MATLAB\n",
    "    clean_data = io.loadmat('data2/clean' + str(d) + '.mat')['X']\n",
    "    filtered_data = io.loadmat('data2/filtered' + str(d) + '.mat')['filteredData']\n",
    "    \n",
    "    # match array sizes, introduces small bias controlled by empirical approximation error\n",
    "    # 1D optimal couplings can still by computed efficiently when array sizes don't match, but implementation is less clean - we omit this at the present\n",
    "    clean_data_set = set([tuple(l) for l in clean_data.tolist()])\n",
    "    clean_filter_cmp_data = np.copy(filtered_data)\n",
    "    for i in range(filtered_data.shape[0]):\n",
    "        x = filtered_data[i,:]\n",
    "        if tuple(x.tolist()) not in clean_data_set:\n",
    "            j = np.random.choice(clean_data.shape[0])\n",
    "            clean_filter_cmp_data[i,:] = clean_data[j,:]\n",
    "    print('prepared clean data')\n",
    "\n",
    "    print('estimating sliced distances')\n",
    "    msw_filtered = msw2_distance_subg(filtered_data, clean_filter_cmp_data, its, samp_sph(d))\n",
    "    MSW_list[d] = msw_filtered    \n",
    "    print('computing W1 lower bound')\n",
    "    W_list[d] = W1_lower_bounds(clean_data, filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
    "\n",
    "plt.plot(dims, [MSW_list[d] for d in dims], label='$\\overline{W}_2 error')\n",
    "plt.plot(dims, [W_list[d] for d in dims], label='W1 error')\n",
    "plt.legend()\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('excess error ($\\ell_2$ distance)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c111c98ab16bb2146b3750ff96300d2ea24b959b05391bbc2c1f7544a5d2c2ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
