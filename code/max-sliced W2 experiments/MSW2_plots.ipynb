{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Optimization.RiemannianBCD import RiemannianBlockCoordinateDescent\n",
    "from Optimization.RiemannianGAS import RiemannianGradientAscentSinkhorn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(x,d,dim=2):\n",
    "    assert dim <= d\n",
    "    assert dim >= 1\n",
    "    assert dim == int(dim)\n",
    "    return x + 2*np.sign(x)*np.array(dim*[1]+(d-dim)*[0])\n",
    "\n",
    "def fragmented_hypercube(n,d,dim):\n",
    "    assert dim <= d\n",
    "    assert dim >= 1\n",
    "    assert dim == int(dim)\n",
    "    \n",
    "    a = (1./n) * np.ones(n)\n",
    "    b = (1./n) * np.ones(n)\n",
    "    \n",
    "    # First measure : uniform on the hypercube\n",
    "    X = np.random.uniform(-1, 1, size=(n,d))\n",
    "\n",
    "    # Second measure : fragmentation\n",
    "    Y = T(np.random.uniform(-1, 1, size=(n,d)), d, dim)\n",
    "    \n",
    "    return a,b,X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitialStiefel(d, k):\n",
    "    U = np.random.randn(d, k)\n",
    "    q, r = np.linalg.qr(U)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y in R^{n x d}\n",
    "# theta in R^{d x 1} (just borrowing existing conventions for higher k)\n",
    "def sliced_objective(X,Y,theta,p=2.0):\n",
    "    theta = theta.flatten()\n",
    "    assert X.shape == Y.shape\n",
    "    assert X.shape[1] == theta.shape[0]\n",
    "    Xtheta = X.dot(theta)\n",
    "    Ytheta = Y.dot(theta)\n",
    "    return np.mean(np.abs(np.sort(Xtheta) - np.sort(Ytheta))**p)**(1/p)\n",
    "\n",
    "#repeated\n",
    "def proj_wp(X, Y, theta, p=2):\n",
    "    N, d = X.shape\n",
    "    theta = theta.flatten()\n",
    "    xproj = np.matmul(X, theta)\n",
    "    yproj = np.matmul(Y, theta)\n",
    "    return np.mean(np.abs((np.sort(xproj) - np.sort(yproj)))**p)**(1/p)\n",
    "\n",
    "def norm(x):\n",
    "    return np.sqrt(sum(x**2))\n",
    "\n",
    "def samp_sph(d):\n",
    "    x = np.random.normal(size = d)\n",
    "    return x/norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subg_step(X, Y, theta, alpha):\n",
    "    N, d = X.shape\n",
    "    theta_X = np.matmul(X, theta)\n",
    "    theta_Y = np.matmul(Y, theta)\n",
    "\n",
    "    X_ind = np.argsort(theta_X)\n",
    "    Y_ind = np.argsort(theta_Y)\n",
    "    grad = 2*np.dot(theta_X[X_ind] - theta_Y[Y_ind], X[X_ind,:] - Y[Y_ind,:])/N\n",
    "    newtheta = (theta + alpha*grad)\n",
    "    return newtheta/norm(newtheta)\n",
    "             \n",
    "def msw2_distance_subg(X, Y, n_step, theta0):\n",
    "    N, d = X.shape\n",
    "    alpha = np.ones(n_step) # constant step size, alternative:/np.sqrt(range(1,n_step + 1))\n",
    "    theta = theta0\n",
    "    wp_dist = np.zeros(n_step)\n",
    "\n",
    "    time_iter = np.zeros(n_step+1)\n",
    "    U_iter = np.zeros((n_step+1,theta.shape[0],1))\n",
    "    U_iter[0,:,:] = theta0[np.newaxis].T\n",
    "\n",
    "    for i in range(n_step):\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        theta = subg_step(X, Y, theta, alpha[i])\n",
    "        wp_dist[i] = proj_wp(X, Y, theta)\n",
    "\n",
    "        toc = time.perf_counter()\n",
    "        time_iter[i + 1] = time_iter[i] + toc - tic\n",
    "        U_iter[i+1,:,:] = theta[np.newaxis].T\n",
    "    return (U_iter, time_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective_iter(X,Y, U_iter):\n",
    "    return np.array([sliced_objective(X,Y,U_iter[i,:,:]) for i in range(U_iter.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100 # Dimension\n",
    "n = 500 # Number of samples\n",
    "k = 1  # Subspace dimension\n",
    "dim = 10 # fragmentation dimension\n",
    "a,b,X,Y = fragmented_hypercube(n,d,dim)\n",
    "rotation = InitialStiefel(d,d)\n",
    "X = X.dot(rotation)\n",
    "Y = Y.dot(rotation)\n",
    "    \n",
    "U0 = InitialStiefel(d, k)\n",
    "\n",
    "gamma = 0.001\n",
    "eta = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run max-sliced W2 computations using three algorithms with varied dimension\n",
    "\n",
    "logs = {}\n",
    "max_iter = 200\n",
    "K = 10\n",
    "\n",
    "for d in [10, 20, 50, 100]:\n",
    "    print(f'd:{d}')\n",
    "\n",
    "    n = 500 # Number of samples\n",
    "    k = 1  # Subspace dimension\n",
    "    dim = 10 # fragmentation dimension\n",
    "    a,b,X,Y = fragmented_hypercube(n,d,dim)\n",
    "    rotation = InitialStiefel(d,d)\n",
    "    X = X.dot(rotation)\n",
    "    Y = Y.dot(rotation)\n",
    "        \n",
    "    U0 = InitialStiefel(d, k)\n",
    "\n",
    "    gamma = 0.001\n",
    "    eta = 0.2\n",
    "\n",
    "    log = []\n",
    "\n",
    "    for i in range(K):\n",
    "        print(i)\n",
    "        params = {'eta':eta, 'tau':gamma/eta, 'max_iter':max_iter, 'threshold':0.1, 'verbose':True}\n",
    "        algo1 = RiemannianGradientAscentSinkhorn(**params)\n",
    "        (U_iter_RAGAS, time_iter_RAGAS) = algo1.run_RAGAS(a, b, X, Y, k, U0)\n",
    "        objective_iter_RAGAS = get_objective_iter(X,Y,U_iter_RAGAS)\n",
    "\n",
    "        params = {'eta':eta, 'tau':gamma, 'max_iter':max_iter, 'threshold':0.1, 'verbose':True}\n",
    "        algo2 = RiemannianBlockCoordinateDescent(**params)\n",
    "        (U_iter_RABCD, time_iter_RABCD) = algo2.run_RABCD(a, b, X, Y, k, U0)\n",
    "        objective_iter_RABCD = get_objective_iter(X,Y,U_iter_RABCD)\n",
    "\n",
    "        (U_iter_subg, time_iter_subg) = msw2_distance_subg(X, Y, max_iter, U0.flatten())\n",
    "        objective_iter_subg = get_objective_iter(X,Y,U_iter_subg)\n",
    "\n",
    "        log.append(((U_iter_RAGAS, objective_iter_RAGAS, time_iter_RAGAS), \n",
    "                    (U_iter_RABCD, objective_iter_RABCD, time_iter_RABCD), \n",
    "                    (U_iter_subg, objective_iter_subg, time_iter_subg)))\n",
    "\n",
    "    logs[d] = log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation Time Plots\n",
    "\n",
    "colors = {}\n",
    "colors[20] = 'g'\n",
    "colors[50] = 'c'\n",
    "colors[100] = 'tab:orange'\n",
    "avgs = {}\n",
    "for d in [20,50,100]:\n",
    "    log = logs[d]\n",
    "    time_iter_RAGAS_avg = np.zeros(log[0][0][2].shape)\n",
    "    time_iter_RABCD_avg = np.zeros(log[0][0][2].shape)\n",
    "    time_iter_subg_avg = np.zeros(log[0][0][2].shape)\n",
    "    for k in range(K):\n",
    "        time_iter_RAGAS_avg += log[k][0][2]\n",
    "        time_iter_RABCD_avg += log[k][1][2]\n",
    "        time_iter_subg_avg += log[k][2][2]\n",
    "    time_iter_RAGAS_avg /= K\n",
    "    time_iter_RABCD_avg /= K\n",
    "    time_iter_subg_avg /= K\n",
    "    avgs[d] = (time_iter_RAGAS_avg, time_iter_RABCD_avg, time_iter_subg_avg)\n",
    "\n",
    "for d in [20,50,100]:\n",
    "    plt.plot(avgs[d][2], '-', label=f'subgradient ascent, d={d}', color=colors[d])\n",
    "for d in [20,50,100]:\n",
    "    plt.plot(avgs[d][1], '--', label=f'RABCD, d={d}', color=colors[d])\n",
    "for d in [20,50,100]:\n",
    "    plt.plot(avgs[d][0], '-.', label=f'RAGAS, d={d}', color=colors[d])\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('time elapsed (seconds)')\n",
    "plt.xlabel('# ascent steps')\n",
    "plt.title('Computation Time for $\\overline{W}_2$ Algorithms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration Complexity Plots\n",
    "\n",
    "colors = {}\n",
    "colors[20] = 'g'\n",
    "colors[50] = 'c'\n",
    "colors[100] = 'tab:orange'\n",
    "avgs = {}\n",
    "for d in [20,50,100]:\n",
    "    log = logs[d]\n",
    "    time_iter_RAGAS_avg = np.zeros(log[0][0][2].shape)\n",
    "    time_iter_RABCD_avg = np.zeros(log[0][0][2].shape)\n",
    "    time_iter_subg_avg = np.zeros(log[0][0][2].shape)\n",
    "    for k in range(K):\n",
    "        time_iter_RAGAS_avg += log[k][0][2]\n",
    "        time_iter_RABCD_avg += log[k][1][2]\n",
    "        time_iter_subg_avg += log[k][2][2]\n",
    "    time_iter_RAGAS_avg /= K\n",
    "    time_iter_RABCD_avg /= K\n",
    "    time_iter_subg_avg /= K\n",
    "    avgs[d] = (time_iter_RAGAS_avg, time_iter_RABCD_avg, time_iter_subg_avg)\n",
    "\n",
    "for d in [20,50,100]:\n",
    "    plt.plot(logs[d][0][2][1], '-', label=f'subgradient ascent, d={d}', color=colors[d])\n",
    "for d in [20,50,100]:\n",
    "    plt.plot(logs[d][0][1][1], '--', label=f'RABCD, d={d}', color=colors[d])\n",
    "for d in [20,50,100]:\n",
    "    plt.plot(logs[d][0][0][1], '-.', label=f'RAGAS, d={d}', color=colors[d])\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('$W_2$ along current slice')\n",
    "plt.xlabel('# ascent steps')\n",
    "plt.title('Iteration Complexity for $\\overline{W}_2$ Algorithms')\n",
    "plt.ylim(1,2.3);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c111c98ab16bb2146b3750ff96300d2ea24b959b05391bbc2c1f7544a5d2c2ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
